{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "mimetype": "text/x-python", "version": "3.5.2", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python"}, "kernelspec": {"display_name": "Python 3.5 (Experimental) with Spark 2.0", "language": "python", "name": "python3-spark20"}}, "cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# Use Deep Learning to Clone Driving Behavior\n\nOverview\n---\nTrain, validate and test a model using Keras to clone driving behavior. The model will output a steering angle to an autonomous vehicle.\nData will be collected by running a car manually in Udacity's Self-Driving Car simulator. The model performance will be tested by running the car in the simulator in autonomous mode.\n\n#### The goals / steps of this project are the following:\n\n* Use the simulator to collect data of good driving behavior\n* Build, a convolution neural network in Keras that predicts steering angles from images\n* Train and validate the model with a training and validation set\n* Test that the model successfully drives around track one without leaving the road\n* Summarize the results with a written report\n\nProject Deliverables\n---\n* `model.py` contains the code for building the keras model based on the [Nvidia architecture](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/). \n* `model.h5` is the trained keras model needed to run the vehicle autonomously.\n* `drive.py` is used to drive the vehicle autonomously in the simulator. It accepts an h5 file as a parameter.\n* `video.mp4` is the video produced from the vehicle's camera view during the autonomous run.\n* `video_rear.mp4` is the video of the same run looking at the vehicle from behind.\n\n#### Importing libraries"}, {"outputs": [], "cell_type": "code", "source": "import os\nimport csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport numpy as np\nimport cv2", "execution_count": null, "metadata": {"collapsed": true}}, {"metadata": {}, "cell_type": "markdown", "source": "#### Read collected data from the simulator\n \nImage data and corresponding steering angle were collected by manually driving the vehicle in the simulator for 3 laps. The vehicle has 3 cameras to record the images - left, center, and right. The steering angles for the images from the left camera were adjusted by 0.1, while the angles for the images from the right camera were adjusted by -0.1 to consider different perspective view from each camera. This resulted in a total of 10,446 (3,482 x 3) images. \n\n The following are the left, center, and right images recorded at the same location:\n\n![Left](https://cloud.githubusercontent.com/assets/10526591/24032112/b7a5090e-0b29-11e7-8d0a-d54419d4c278.jpg)\n![Center](https://cloud.githubusercontent.com/assets/10526591/24032113/b7c63c14-0b29-11e7-878f-663096e30985.jpg)\n![Right](https://cloud.githubusercontent.com/assets/10526591/24032111/b7843300-0b29-11e7-9b89-6f4da4eaf539.jpg)\n"}, {"outputs": [], "cell_type": "code", "source": "from io import StringIO\nimport requests\nimport json\nimport pandas as pd\n\n# @hidden_cell\n# This function accesses a file in your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\ndef get_object_storage_file_with_credentials_83e0455de72a4323ae30b7f17b282631(container, filename):\n    \"\"\"This functions returns a StringIO object containing\n    the file content from Bluemix Object Storage.\"\"\"\n\n    url1 = ''.join(['https://identity.open.softlayer.com', '/v3/auth/tokens'])\n    data = {'auth': {'identity': {'methods': ['password'],\n            'password': {'user': {'name': 'member_33c4989825a36840cc560705f473ac4e52e87422','domain': {'id': 'bf05c10c9a8f4077b8577c31fce9138b'},\n            'password': 'g.I?Pse5Z2VDG7Z5'}}}}}\n    headers1 = {'Content-Type': 'application/json'}\n    resp1 = requests.post(url=url1, data=json.dumps(data), headers=headers1)\n    resp1_body = resp1.json()\n    for e1 in resp1_body['token']['catalog']:\n        if(e1['type']=='object-store'):\n            for e2 in e1['endpoints']:\n                        if(e2['interface']=='public'and e2['region']=='dallas'):\n                            url2 = ''.join([e2['url'],'/', container, '/', filename])\n    s_subject_token = resp1.headers['x-subject-token']\n    headers2 = {'X-Auth-Token': s_subject_token, 'accept': 'application/json'}\n    resp2 = requests.get(url=url2, headers=headers2)\n    return StringIO(resp2.text)\n\n# Your data file was loaded into a StringIO object and you can process the data.\n# Please read the documentation of pandas to learn more about your possibilities to load your data.\n# pandas documentation: http://pandas.pydata.org/pandas-docs/stable/io.html\ndata_1 = get_object_storage_file_with_credentials_83e0455de72a4323ae30b7f17b282631('SelfDrivingCarDemo', 'data.zip')\n", "execution_count": 9, "metadata": {"collapsed": false}}, {"outputs": [], "cell_type": "code", "source": "lines = []\nwith open('./data/driving_log.csv') as csvfile:\n    reader = csv.reader(csvfile)\n    for line in reader:\n        lines.append(line)", "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": "shuffle(lines)\ntrain_data, validation_data = train_test_split(lines, test_size=0.2)", "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": "images = []\nangles = []\nfor line in train_data:\n    # use left, center, right camera images\n    angle_correction = 0.1\n    for i in range(3):\n        name = './data/IMG/' + line[i].split('\\\\')[-1]\n        image = cv2.imread(name)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        images.append(image)\n        angle = float(line[3])\n        # adjust steering wheel angle for different camera views\n    angles.append(angle)\n    angles.append(angle + angle_correction)\n    angles.append(angle - angle_correction)\nprint(\"done loading {} images\".format(len(images)))\nimages_with_flipped = []\nangles_with_flipped = []\n\n# add flipped images and angles\nfor image, angle in zip(images, angles):\n    images_with_flipped.append(image)\n    angles_with_flipped.append(angle)\n    flipped_image = np.fliplr(image)\n    flipped_angle = -angle\n    images_with_flipped.append(flipped_image)\n    angles_with_flipped.append(flipped_angle)\nprint(\"done flipping and loading {} images\".format(len(images_with_flipped)))", "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": "# prepare validation set\nval_images = []\nval_angles = []\nfor line in validation_data:\n    # use left, center, right camera images\n    angle_correction = 0.1\n    for i in range(3):\n        name = './data/IMG/' + line[i].split('\\\\')[-1]\n        image = cv2.imread(name)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        val_images.append(image)\n        angle = float(line[3])\n        # adjust steering wheel angle for different camera views\n    val_angles.append(angle)\n    val_angles.append(angle + angle_correction)\n    val_angles.append(angle - angle_correction)\nprint(\"done loading {} validation images\".format(len(val_images)))\n\nX_train = np.array(images_with_flipped)\ny_train = np.array(angles_with_flipped)\nX_val = np.array(val_images)\ny_val = np.array(val_angles)\n", "execution_count": null, "metadata": {"collapsed": true}}, {"metadata": {}, "cell_type": "markdown", "source": "Model Architecture\n---\nI based my model on the [Nvidia architecture](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/). It includes a normalization layer, 5 convolutional layers,1 dropout layer, and finally 3 fully connected layers. \nThrough this model, the vehicle was able to drive autonomously around the track without leaving the road or diving into water and making smooth turns around corners."}, {"outputs": [], "cell_type": "code", "source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Lambda, Dropout\nfrom keras.layers.convolutional import Convolution2D, Cropping2D\n\nmodel = Sequential()\nmodel.add(Lambda(lambda x: x / 255.0 - 0.5, input_shape=(160, 320, 3)))\nmodel.add(Cropping2D(cropping=((70, 25), (0, 0))))\nmodel.add(Convolution2D(24, 5, 5, subsample=(2, 2), activation='relu'))\nmodel.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='relu'))\nmodel.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='relu'))\nmodel.add(Convolution2D(64, 3, 3, activation='relu'))\nmodel.add(Convolution2D(64, 3, 3, activation='relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Flatten())\nmodel.add(Dense(100))\nmodel.add(Dense(50))\nmodel.add(Dense(10))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse')\nmodel.fit(X_train, y_train, validation_data = (X_val, y_val), nb_epoch=5)", "execution_count": null, "metadata": {"collapsed": true}}, {"outputs": [], "cell_type": "code", "source": "model.save('model.h5')", "execution_count": null, "metadata": {"collapsed": true}}, {"metadata": {}, "cell_type": "markdown", "source": "Results\n---\nView the **[video](https://www.youtube.com/watch?v=MB-ii0qzUmQ)** on Youtube"}, {"outputs": [{"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "\n<video width=\"640\" height=\"480\" controls>\n  <source src=\"https://github.com/aruizga7/Self-Driving-Car-in-DSX/raw/master/5.%20Behavioral%20cloning/Self%20Driving%20Behavior%20Cloning.mp4\">\n</video>\n"}, "metadata": {}}], "cell_type": "code", "source": "from IPython.display import HTML\nHTML(\"\"\"\n<video width=\"640\" height=\"480\" controls>\n  <source src=\"https://github.com/aruizga7/Self-Driving-Car-in-DSX/raw/master/5.%20Behavioral%20cloning/Self%20Driving%20Behavior%20Cloning.mp4\">\n</video>\n\"\"\")", "execution_count": 16, "metadata": {"collapsed": false}}, {"metadata": {}, "cell_type": "markdown", "source": "####  How to run the model in autonomous mode\nUsing the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing \n```\npython drive.py model.h5\n```\n\n####  Model Pipeline\n\nThe model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.\n"}, {"outputs": [], "cell_type": "code", "source": "", "execution_count": null, "metadata": {"collapsed": true}}], "nbformat": 4, "nbformat_minor": 0}